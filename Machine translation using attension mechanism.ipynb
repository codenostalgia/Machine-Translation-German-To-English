{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "import keras\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from random import *\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import pydot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(ger_length)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights')\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    " \n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    \n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    \n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    \n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    \n",
    "    sentences= []\n",
    "    \n",
    "    for i in range(len(source)):\n",
    "\n",
    "        prediction = model.predict(source, verbose=0)[i]\n",
    "        integers = [argmax(vector) for vector in prediction]\n",
    "        target = list()\n",
    "        for i in integers:\n",
    "            word = word_for_id(i, tokenizer)\n",
    "            if word is None:\n",
    "                break\n",
    "            target.append(word)\n",
    "        sentence= ' '.join(target)\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Softmax activation function.\n",
    "def softmax(x, axis=1):\n",
    "    \n",
    "# Arguments -\n",
    "# x : Tensor.\n",
    "# axis: Integer, axis along which the softmax normalization is applied.\n",
    "\n",
    "# Returns-\n",
    "# Tensor, output of softmax transformation.\n",
    "\n",
    "# Raises-\n",
    "# ValueError: In case `dim(x) == 1`.\n",
    "\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "\n",
    "# train has 9000 examples and test has 1000 examples\n",
    "train = load_clean_sentences('E:\\Machine translation project\\Train_data.pkl')\n",
    "test = load_clean_sentences('E:\\Machine translation project\\Test_data.pkl')\n",
    "dataset = train +test\n",
    "\n",
    "#Shuffling dataset as the sentences present in test data are larger in size\n",
    "shuffle(dataset)\n",
    "dataset = np.array(dataset)\n",
    "\n",
    "# train on 9000 examples\n",
    "train= dataset[:9000,:]\n",
    "\n",
    "#test on remaining  1000 examples\n",
    "test = dataset[9000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2214\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 3526\n",
      "German Max Length: 9\n",
      "(9000, 9, 3526)\n",
      "(9000, 5, 2214)\n",
      "(1000, 9, 3526)\n",
      "(1000, 5, 2214)\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    "\n",
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "Xoh_train = encode_output(trainX, ger_vocab_size)\n",
    "print(Xoh_train.shape)\n",
    "\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "Yoh_train = encode_output(trainY, eng_vocab_size)\n",
    "print(Yoh_train.shape)\n",
    "\n",
    "\n",
    "# prepare testing data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "Xoh_test = encode_output(testX, ger_vocab_size)\n",
    "print(Xoh_test.shape)\n",
    "\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "Yoh_test = encode_output(testY, eng_vocab_size)\n",
    "print(Yoh_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "# \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "# Input Arguments:\n",
    "# a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "# s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "# Returns:\n",
    "# context -- context vector, input of the next (post-attention) LSTM cell\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "\n",
    "\n",
    "    s_prev = repeator(s_prev)\n",
    "    concat = concatenator([a,s_prev])\n",
    "    e = densor1(concat)\n",
    "   \n",
    "    energies = densor2(e)\n",
    "   \n",
    "    alphas = activator(energies)\n",
    "    \n",
    "    context = dotor([alphas,a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 32 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
    "n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n",
    "\n",
    "#  post attention LSTM cell.  \n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # post-attention LSTM \n",
    "output_layer = Dense( eng_vocab_size, activation=softmax )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function buids the model using the required layers.\n",
    "\n",
    "# Input Arguments:\n",
    "# Tx --                 length of the input sequence\n",
    "# Ty --                 length of the output sequence\n",
    "# n_a --                hidden state size of the Bi-LSTM\n",
    "# n_s --                hidden state size of the post-attention LSTM\n",
    "\n",
    "# human_vocab_size --   size of the python dictionary \"human_vocab\"\n",
    "# machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "# Returns:\n",
    "# model -- Keras model instance\n",
    "    \n",
    "\n",
    "def model( src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_a , n_s):\n",
    "    \n",
    "\n",
    "    X = Input(shape=(src_timesteps , src_vocab))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    a = Bidirectional(LSTM(units=n_a, return_sequences=True))(X)\n",
    "    \n",
    "    for t in range(tar_timesteps):\n",
    "    \n",
    "        context = one_step_attention(a,s)\n",
    "        s, _, c = post_activation_LSTM_cell(context,initial_state=[s,c])\n",
    "        \n",
    "        out =output_layer(s)\n",
    "    \n",
    "        outputs.append(out)\n",
    "    \n",
    "    model = Model(inputs=[X,s0,c0] , outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 9, 3526)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 9, 64)        911104      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 9, 64)        0           s0[0][0]                         \n",
      "                                                                 lstm_5[20][0]                    \n",
      "                                                                 lstm_5[21][0]                    \n",
      "                                                                 lstm_5[22][0]                    \n",
      "                                                                 lstm_5[23][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 128)       0           bidirectional_9[0][0]            \n",
      "                                                                 repeat_vector_1[25][0]           \n",
      "                                                                 bidirectional_9[0][0]            \n",
      "                                                                 repeat_vector_1[26][0]           \n",
      "                                                                 bidirectional_9[0][0]            \n",
      "                                                                 repeat_vector_1[27][0]           \n",
      "                                                                 bidirectional_9[0][0]            \n",
      "                                                                 repeat_vector_1[28][0]           \n",
      "                                                                 bidirectional_9[0][0]            \n",
      "                                                                 repeat_vector_1[29][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 9, 10)        1290        concatenate_1[25][0]             \n",
      "                                                                 concatenate_1[26][0]             \n",
      "                                                                 concatenate_1[27][0]             \n",
      "                                                                 concatenate_1[28][0]             \n",
      "                                                                 concatenate_1[29][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 9, 1)         11          dense_2[25][0]                   \n",
      "                                                                 dense_2[26][0]                   \n",
      "                                                                 dense_2[27][0]                   \n",
      "                                                                 dense_2[28][0]                   \n",
      "                                                                 dense_2[29][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 9, 1)         0           dense_3[25][0]                   \n",
      "                                                                 dense_3[26][0]                   \n",
      "                                                                 dense_3[27][0]                   \n",
      "                                                                 dense_3[28][0]                   \n",
      "                                                                 dense_3[29][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[25][0]         \n",
      "                                                                 bidirectional_9[0][0]            \n",
      "                                                                 attention_weights[26][0]         \n",
      "                                                                 bidirectional_9[0][0]            \n",
      "                                                                 attention_weights[27][0]         \n",
      "                                                                 bidirectional_9[0][0]            \n",
      "                                                                 attention_weights[28][0]         \n",
      "                                                                 bidirectional_9[0][0]            \n",
      "                                                                 attention_weights[29][0]         \n",
      "                                                                 bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, 64), (None,  33024       dot_1[25][0]                     \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[26][0]                     \n",
      "                                                                 lstm_5[20][0]                    \n",
      "                                                                 lstm_5[20][2]                    \n",
      "                                                                 dot_1[27][0]                     \n",
      "                                                                 lstm_5[21][0]                    \n",
      "                                                                 lstm_5[21][2]                    \n",
      "                                                                 dot_1[28][0]                     \n",
      "                                                                 lstm_5[22][0]                    \n",
      "                                                                 lstm_5[22][2]                    \n",
      "                                                                 dot_1[29][0]                     \n",
      "                                                                 lstm_5[23][0]                    \n",
      "                                                                 lstm_5[23][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2214)         143910      lstm_5[20][0]                    \n",
      "                                                                 lstm_5[21][0]                    \n",
      "                                                                 lstm_5[22][0]                    \n",
      "                                                                 lstm_5[23][0]                    \n",
      "                                                                 lstm_5[24][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,089,339\n",
      "Trainable params: 1,089,339\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=0.1)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy')\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((9000, n_s))\n",
    "c0 = np.zeros((9000, n_s))\n",
    "outputs = list(Yoh_train.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 21.3591 - dense_5_loss: 0.4104\n",
      "Epoch 2/50\n",
      "9000/9000 [==============================] - 7s 735us/step - loss: 18.6681 - dense_5_loss: 0.0184\n",
      "Epoch 3/50\n",
      "9000/9000 [==============================] - 7s 729us/step - loss: 18.2446 - dense_5_loss: 0.0387\n",
      "Epoch 4/50\n",
      "9000/9000 [==============================] - 7s 742us/step - loss: 15.3955 - dense_5_loss: 0.0240\n",
      "Epoch 5/50\n",
      "9000/9000 [==============================] - 7s 737us/step - loss: 13.6616 - dense_5_loss: 0.0190\n",
      "Epoch 6/50\n",
      "9000/9000 [==============================] - 7s 744us/step - loss: 12.6987 - dense_5_loss: 0.0093\n",
      "Epoch 7/50\n",
      "9000/9000 [==============================] - 7s 741us/step - loss: 11.9074 - dense_5_loss: 0.0126\n",
      "Epoch 8/50\n",
      "9000/9000 [==============================] - 7s 731us/step - loss: 11.3586 - dense_5_loss: 0.0119\n",
      "Epoch 9/50\n",
      "9000/9000 [==============================] - 7s 733us/step - loss: 10.8713 - dense_5_loss: 0.0096\n",
      "Epoch 10/50\n",
      "9000/9000 [==============================] - 7s 735us/step - loss: 10.7119 - dense_5_loss: 0.0103\n",
      "Epoch 11/50\n",
      "9000/9000 [==============================] - 7s 724us/step - loss: 10.3656 - dense_5_loss: 0.0086\n",
      "Epoch 12/50\n",
      "9000/9000 [==============================] - 7s 729us/step - loss: 10.0247 - dense_5_loss: 0.0093\n",
      "Epoch 13/50\n",
      "9000/9000 [==============================] - 7s 725us/step - loss: 9.7568 - dense_5_loss: 0.0071\n",
      "Epoch 14/50\n",
      "9000/9000 [==============================] - 7s 732us/step - loss: 9.4774 - dense_5_loss: 0.0071\n",
      "Epoch 15/50\n",
      "9000/9000 [==============================] - 7s 725us/step - loss: 9.0462 - dense_5_loss: 0.0062\n",
      "Epoch 16/50\n",
      "9000/9000 [==============================] - 7s 726us/step - loss: 8.8862 - dense_5_loss: 0.0079\n",
      "Epoch 17/50\n",
      "9000/9000 [==============================] - 7s 725us/step - loss: 8.6167 - dense_5_loss: 0.0069\n",
      "Epoch 18/50\n",
      "9000/9000 [==============================] - 7s 736us/step - loss: 8.3590 - dense_5_loss: 0.0066\n",
      "Epoch 19/50\n",
      "9000/9000 [==============================] - 6s 721us/step - loss: 8.0959 - dense_5_loss: 0.0075\n",
      "Epoch 20/50\n",
      "9000/9000 [==============================] - 6s 716us/step - loss: 7.8443 - dense_5_loss: 0.0048\n",
      "Epoch 21/50\n",
      "9000/9000 [==============================] - 6s 720us/step - loss: 7.7638 - dense_5_loss: 0.0063\n",
      "Epoch 22/50\n",
      "9000/9000 [==============================] - 7s 767us/step - loss: 7.6805 - dense_5_loss: 0.00600s - loss: 7.6624 - dense_5_loss\n",
      "Epoch 23/50\n",
      "9000/9000 [==============================] - 7s 793us/step - loss: 7.5401 - dense_5_loss: 0.0059\n",
      "Epoch 24/50\n",
      "9000/9000 [==============================] - 7s 745us/step - loss: 7.4068 - dense_5_loss: 0.0063\n",
      "Epoch 25/50\n",
      "9000/9000 [==============================] - 7s 780us/step - loss: 7.2763 - dense_5_loss: 0.0052\n",
      "Epoch 26/50\n",
      "9000/9000 [==============================] - 6s 722us/step - loss: 7.2322 - dense_5_loss: 0.0050\n",
      "Epoch 27/50\n",
      "9000/9000 [==============================] - 6s 721us/step - loss: 7.1536 - dense_5_loss: 0.0058\n",
      "Epoch 28/50\n",
      "9000/9000 [==============================] - 7s 725us/step - loss: 7.3366 - dense_5_loss: 0.0065\n",
      "Epoch 29/50\n",
      "9000/9000 [==============================] - 7s 723us/step - loss: 6.9999 - dense_5_loss: 0.0046\n",
      "Epoch 30/50\n",
      "9000/9000 [==============================] - 7s 728us/step - loss: 6.9085 - dense_5_loss: 0.0057\n",
      "Epoch 31/50\n",
      "9000/9000 [==============================] - 7s 732us/step - loss: 6.7519 - dense_5_loss: 0.0069\n",
      "Epoch 32/50\n",
      "9000/9000 [==============================] - 7s 730us/step - loss: 6.6838 - dense_5_loss: 0.0059\n",
      "Epoch 33/50\n",
      "9000/9000 [==============================] - 7s 735us/step - loss: 6.5651 - dense_5_loss: 0.0070\n",
      "Epoch 34/50\n",
      "9000/9000 [==============================] - 6s 718us/step - loss: 6.3725 - dense_5_loss: 0.0063\n",
      "Epoch 35/50\n",
      "9000/9000 [==============================] - 7s 727us/step - loss: 6.3399 - dense_5_loss: 0.0045\n",
      "Epoch 36/50\n",
      "9000/9000 [==============================] - 6s 719us/step - loss: 7.3785 - dense_5_loss: 0.0130\n",
      "Epoch 37/50\n",
      "9000/9000 [==============================] - 6s 722us/step - loss: 6.7379 - dense_5_loss: 0.0074\n",
      "Epoch 38/50\n",
      "9000/9000 [==============================] - 7s 723us/step - loss: 6.4172 - dense_5_loss: 0.0055\n",
      "Epoch 39/50\n",
      "9000/9000 [==============================] - 7s 731us/step - loss: 6.2450 - dense_5_loss: 0.0046\n",
      "Epoch 40/50\n",
      "9000/9000 [==============================] - 6s 720us/step - loss: 6.1016 - dense_5_loss: 0.0044\n",
      "Epoch 41/50\n",
      "9000/9000 [==============================] - 6s 717us/step - loss: 6.0502 - dense_5_loss: 0.0069\n",
      "Epoch 42/50\n",
      "9000/9000 [==============================] - 7s 725us/step - loss: 5.8962 - dense_5_loss: 0.0055\n",
      "Epoch 43/50\n",
      "9000/9000 [==============================] - 7s 725us/step - loss: 5.8152 - dense_5_loss: 0.0051\n",
      "Epoch 44/50\n",
      "9000/9000 [==============================] - 7s 787us/step - loss: 5.7565 - dense_5_loss: 0.0046\n",
      "Epoch 45/50\n",
      "9000/9000 [==============================] - 7s 757us/step - loss: 5.6336 - dense_5_loss: 0.0051\n",
      "Epoch 46/50\n",
      "9000/9000 [==============================] - 7s 768us/step - loss: 5.5362 - dense_5_loss: 0.0049\n",
      "Epoch 47/50\n",
      "9000/9000 [==============================] - 7s 812us/step - loss: 5.5514 - dense_5_loss: 0.0059\n",
      "Epoch 48/50\n",
      "9000/9000 [==============================] - 6s 714us/step - loss: 5.6459 - dense_5_loss: 0.0064\n",
      "Epoch 49/50\n",
      "9000/9000 [==============================] - 7s 728us/step - loss: 5.5722 - dense_5_loss: 0.0062\n",
      "Epoch 50/50\n",
      "9000/9000 [==============================] - 7s 733us/step - loss: 5.6139 - dense_5_loss: 0.0068\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([Xoh_train, s0, c0], outputs, epochs=50 , batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ce29f869b0>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting learning curve\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[62,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting output over an input.\n",
    "\n",
    "a = encode_sequences(ger_tokenizer, ger_length, [ 'geh'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[7.86866804e-06, 2.87491506e-08, 1.54940990e-05, ...,\n",
       "         1.42489805e-11, 1.18787037e-11, 1.19909733e-11]], dtype=float32),\n",
       " array([[2.6664367e-02, 4.5792625e-04, 1.1609279e-06, ..., 1.9763270e-10,\n",
       "         1.6798546e-10, 1.6205803e-10]], dtype=float32),\n",
       " array([[9.7948372e-01, 8.3463019e-06, 2.6143532e-08, ..., 1.5378759e-11,\n",
       "         1.3973919e-11, 1.7836134e-11]], dtype=float32),\n",
       " array([[9.8613495e-01, 8.9586856e-06, 4.6248432e-11, ..., 5.0200535e-16,\n",
       "         4.5589696e-16, 4.3257411e-16]], dtype=float32),\n",
       " array([[9.9997807e-01, 4.6859254e-09, 2.3359342e-16, ..., 5.5418160e-17,\n",
       "         5.1923292e-17, 5.8133142e-17]], dtype=float32)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = encode_output(a, ger_vocab_size)\n",
    "b = model.predict([a,s0,c0])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'out', '']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction\n",
    "\n",
    "b = predict_sequence(model, eng_tokenizer , [a,s0,c0])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
